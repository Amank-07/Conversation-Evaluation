# ğŸ§  Ahoum Conversation Evaluation Benchmark

A production-ready benchmark system for evaluating conversational AI models across multiple dimensions including linguistic quality, pragmatics, safety, emotion, and behavioral traits.

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Features](#-features)
- [Goals & Requirements](#-goals--requirements)
- [System Architecture](#-system-architecture)
- [Benchmark Dataset](#-benchmark-dataset)
- [Installation](#-installation)
- [Usage](#-usage)
- [Technical Challenges & Solutions](#-technical-challenges--solutions)
- [Project Structure](#-project-structure)
- [Deliverables](#-deliverables)

---

## ğŸ“Œ Project Overview

This project implements a **production-ready benchmark system** for evaluating conversational AI models across **300+ distinct facets** covering:

- **Linguistic Quality** â€” Grammar, coherence, and language proficiency
- **Pragmatics** â€” Contextual understanding and appropriateness
- **Safety** â€” Content moderation and harmful content detection
- **Emotion** â€” Emotional awareness and response quality
- **Behavioral & Psychological Traits** â€” Personality indicators and psychological patterns

Each conversation turn is scored on a **five-level ordered scale** with an associated **confidence score**. The architecture is designed to **scale beyond 5000 facets without any redesign**, ensuring future extensibility and maintainability.

---

## âœ¨ Features

- âœ… Multi-faceted evaluation system (300+ facets, extensible to 5000+)
- âœ… Five-level ordered scoring with confidence metrics
- âœ… Fault-tolerant execution with checkpoint recovery
- âœ… Support for open-weights models (â‰¤16B parameters)
- âœ… Dockerized deployment for consistent environments
- âœ… Interactive Streamlit UI for result visualization
- âœ… Robust JSON validation and error handling
- âœ… Dynamic batching for efficient processing

---

## ğŸ¯ Goals & Requirements

| Requirement | Status |
|------------|--------|
| Evaluate every conversation turn | âœ… |
| 300+ facets | âœ… |
| Five-level ordered scoring | âœ… |
| Confidence per score | âœ… |
| Supports â‰¥ 5000 facets | âœ… |
| Open-weights model â‰¤16B | âœ… (Qwen / Llama-class models) |
| No one-shot prompt | âœ… |
| Fault-tolerant execution | âœ… |
| Dockerized baseline | âœ… |
| Sample UI (Streamlit) | âœ… |

---

## ğŸ§± System Architecture

```
Facets CSV
    â†“
Loader â†’ Preprocessor â†’ Facet Engine
    â†“
Batch Controller
    â†“
LLM Scorer (Open-weight model)
    â†“
JSON Validator & Extractor
    â†“
Checkpoint Engine
    â†“
CSV / JSON Results
```

### Core Components

- **Loader Module** â€” Efficiently loads and parses facet dataset
- **Preprocessor** â€” Cleans and enriches facet metadata
- **Facet Engine** â€” Dynamically manages and batches facets for scalable processing
- **Scorer** â€” Executes evaluation with confidence scoring using open-weights models
- **Main Pipeline** â€” Orchestrates the complete benchmark workflow
- **Checkpoint System** â€” Crash-safe recovery mechanism for long-running evaluations
- **UI Dashboard** â€” Streamlit-based visualization and analysis interface

---

## ğŸ§ª Benchmark Dataset

The benchmark includes **50+ conversations** covering diverse scenarios:

- Emotional distress and support
- Hostility and safety-critical cases
- Happiness and optimistic interactions
- Technical problem-solving scenarios
- Professional conflict resolution
- Mental health indicators

Each conversation is evaluated against **all configured facets**, providing comprehensive multi-dimensional analysis.

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Docker (optional, for containerized deployment)
- GPU recommended (optional, for faster inference)

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ahoum-read-me
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify installation**
   ```bash
   python main.py --help
   ```

---

## ğŸš€ Usage

### Local Execution

Run the benchmark pipeline:

```bash
python main.py
```

Results will be saved as:
- `benchmark_results.csv`
- `benchmark_results.json`

### Docker Deployment

**Build the image:**
```bash
docker build -t ahoum-benchmark .
```

**Run the container:**
```bash
docker run -p 8501:8501 ahoum-benchmark
```

### Streamlit UI

Launch the interactive dashboard:

```bash
streamlit run ui/app.py
```

Access the UI at `http://localhost:8501`

---

## ğŸ§— Technical Challenges & Solutions

### Challenge 1: Extremely Long Execution Time

**Problem:**  
Evaluating hundreds of facets across dozens of conversations caused frequent disconnections and total progress loss in cloud environments.

**Solution:**  
Implemented a **checkpoint-based execution engine** that saves progress after every conversation. The pipeline automatically resumes from the last completed conversation in case of interruptions, ensuring no work is lost.

---

### Challenge 2: Inconsistent JSON Output from the Model

**Problem:**  
Language models sometimes returned malformed, partial, or non-standard JSON responses, causing pipeline failures.

**Solution:**  
Built a **robust extraction and validation layer** that:
- Isolates valid JSON from model responses
- Enforces schema correctness
- Safely skips corrupted rows with error logging
- Provides fallback mechanisms for edge cases

---

### Challenge 3: Scaling Beyond 300 Facets

**Problem:**  
Hard-coded logic and static configurations fail at large scale, making it difficult to extend beyond initial requirements.

**Solution:**  
The **Facet Engine** dynamically loads and batches facets from configuration files. The architecture supports **5000+ facets without any redesign**, using scalable data structures and processing pipelines.

---

### Challenge 4: Preventing One-Shot Prompting

**Problem:**  
One-shot prompting approaches fail at scale and violate assignment constraints for large facet evaluations.

**Solution:**  
Facets are evaluated in **controlled batches** with optimized prompt engineering, ensuring stability, compliance, and consistent performance across all evaluation dimensions.

---

### Challenge 5: Hardware Limitations

**Problem:**  
Limited memory and runtime constraints in cloud notebook environments made it difficult to complete full benchmark runs.

**Solution:**  
Dynamic batching combined with checkpoint recovery enables reliable execution across diverse hardware configurations (CPU, GPU, cloud instances). The system adapts batch sizes based on available resources.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ loader.py              # Facet dataset loader
â”œâ”€â”€ preprocessor.py        # Metadata preprocessing
â”œâ”€â”€ facet_engine.py        # Dynamic facet management
â”œâ”€â”€ scorer.py              # LLM-based evaluation scorer
â”œâ”€â”€ main.py                # Main pipeline orchestrator
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # Container configuration
â”œâ”€â”€ checkpoints/           # Checkpoint storage
â”œâ”€â”€ ui/                    # Streamlit dashboard
â”‚   â””â”€â”€ app.py
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ“¦ Deliverables

- âœ… Full source code with comprehensive documentation
- âœ… Dockerized pipeline for reproducible deployments
- âœ… Streamlit UI for interactive result analysis
- âœ… 50+ conversation benchmark dataset
- âœ… CSV and JSON evaluation results
- âœ… Fault-tolerant benchmark engine with checkpoint recovery
- âœ… Scalable architecture supporting 5000+ facets

---

## ğŸ§¾ Conclusion

This benchmark framework provides a **scalable, reliable, and production-ready solution** for deep conversational evaluation across linguistic, emotional, and safety dimensions. The system meets and exceeds all assignment requirements while providing a foundation for future research and development in conversational AI evaluation.

---

## ğŸ“„ License

[Specify your license here]

## ğŸ¤ Contributing

[Contributing guidelines, if applicable]

## ğŸ“§ Contact

[Contact information or issue tracker]
